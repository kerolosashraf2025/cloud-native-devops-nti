name: Terraform Infrastructure

on:
  workflow_dispatch:
    inputs:
      action:
        description: "Choose Terraform action"
        required: true
        default: apply
        type: choice
        options:
          - apply
          - destroy

jobs:
  terraform:
    name: Terraform Infrastructure
    runs-on: self-hosted

    defaults:
      run:
        shell: bash
        working-directory: terraform

    env:
      AWS_REGION: eu-west-1
      CLUSTER_NAME: nti-nonprod-eks
      ECR_REPO_NAME_APP: cloud-native-sample-app
      ECR_REPO_NAME_WRITER: cloud-native-mongo-writer

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.6

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        run: terraform init

      # ======================================================
      # APPLY FLOW
      # ======================================================
      - name: Auto Import ECR Repos if exist
        if: ${{ github.event.inputs.action == 'apply' }}
        run: |
          set -e

          echo "=========================================="
          echo "Checking ECR repos in AWS..."
          echo "=========================================="

          check_and_import_repo () {
            REPO_NAME=$1
            TF_RESOURCE=$2

            echo "------------------------------------------"
            echo "Repo: $REPO_NAME"
            echo "Terraform Resource: $TF_RESOURCE"
            echo "------------------------------------------"

            if aws ecr describe-repositories --repository-names "$REPO_NAME" --region "$AWS_REGION" >/dev/null 2>&1; then
              echo "Repo exists in AWS: $REPO_NAME"
            else
              echo "Repo does not exist in AWS. Terraform will create it."
              return
            fi

            if terraform state list | grep -q "$TF_RESOURCE"; then
              echo "Repo already exists in Terraform state: $TF_RESOURCE"
            else
              echo "Repo missing in state -> importing..."
              terraform import "$TF_RESOURCE" "$REPO_NAME"
              echo "Import completed."
            fi
          }

          check_and_import_repo "$ECR_REPO_NAME_APP" "aws_ecr_repository.sample_app"
          check_and_import_repo "$ECR_REPO_NAME_WRITER" "aws_ecr_repository.mongo_writer"

      - name: Terraform Plan
        if: ${{ github.event.inputs.action == 'apply' }}
        run: terraform plan -lock-timeout=5m -var-file=nonprod.tfvars

      - name: Terraform Apply
        if: ${{ github.event.inputs.action == 'apply' }}
        run: terraform apply -lock-timeout=5m -auto-approve -var-file=nonprod.tfvars


      # ======================================================
      # DESTROY FLOW (FULL CLEANUP NO HANG)
      # ======================================================
      - name: Install kubectl (if missing)
        if: ${{ github.event.inputs.action == 'destroy' }}
        run: |
          set -e
          if ! command -v kubectl >/dev/null 2>&1; then
            echo "Installing kubectl..."
            curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            chmod +x kubectl
            sudo mv kubectl /usr/local/bin/kubectl
          fi
          kubectl version --client || true

      - name: Install Helm (if missing)
        if: ${{ github.event.inputs.action == 'destroy' }}
        run: |
          set -e
          if ! command -v helm >/dev/null 2>&1; then
            echo "Installing Helm..."
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | sudo bash
          fi
          helm version || true

      - name: Update kubeconfig (safe)
        if: ${{ github.event.inputs.action == 'destroy' }}
        run: |
          set +e
          echo "=========================================="
          echo "Updating kubeconfig..."
          echo "=========================================="
          timeout 60 aws eks update-kubeconfig --region "$AWS_REGION" --name "$CLUSTER_NAME"
          echo "=========================================="
          echo "Testing cluster connectivity..."
          echo "=========================================="
          timeout 30 kubectl get nodes --request-timeout=20s
          echo "Cluster reachable ✅"

      - name: Full Cleanup Kubernetes Resources (Safe - No Hanging)
        if: ${{ github.event.inputs.action == 'destroy' }}
        run: |
          set +e

          echo "=========================================="
          echo "Current Services (before cleanup)"
          echo "=========================================="
          timeout 30 kubectl get svc -A --request-timeout=20s || true

          echo "=========================================="
          echo "Uninstalling Helm releases (if exist)..."
          echo "=========================================="
          timeout 120 helm uninstall ingress-nginx -n ingress-nginx || true
          timeout 120 helm uninstall argocd -n argocd || true
          timeout 120 helm uninstall datadog -n datadog || true
          timeout 120 helm uninstall external-secrets -n external-secrets || true

          echo "=========================================="
          echo "Deleting ArgoCD Applications (CRDs objects)"
          echo "=========================================="
          timeout 60 kubectl delete applications.argoproj.io --all -n argocd --ignore-not-found=true --request-timeout=20s || true

          echo "=========================================="
          echo "Deleting ExternalSecrets + ClusterSecretStores..."
          echo "=========================================="
          timeout 60 kubectl delete externalsecret --all -A --ignore-not-found=true --request-timeout=20s || true
          timeout 60 kubectl delete clustersecretstore --all --ignore-not-found=true --request-timeout=20s || true

          echo "=========================================="
          echo "Deleting ALL Ingress resources..."
          echo "=========================================="
          timeout 60 kubectl delete ingress --all -A --ignore-not-found=true --request-timeout=20s || true

          echo "=========================================="
          echo "Deleting ALL Services of type LoadBalancer..."
          echo "=========================================="
          timeout 60 kubectl get svc -A --no-headers --request-timeout=20s 2>/dev/null | while read ns name rest; do
            TYPE=$(timeout 20 kubectl get svc "$name" -n "$ns" -o jsonpath='{.spec.type}' --request-timeout=10s 2>/dev/null)
            if [ "$TYPE" = "LoadBalancer" ]; then
              echo "Deleting LoadBalancer service $name in namespace $ns ..."
              timeout 60 kubectl delete svc "$name" -n "$ns" --ignore-not-found=true --request-timeout=20s || true
            fi
          done || true

          echo "=========================================="
          echo "Deleting Namespaces (force cleanup)"
          echo "=========================================="
          timeout 60 kubectl delete ns ingress-nginx --ignore-not-found=true --request-timeout=20s || true
          timeout 60 kubectl delete ns argocd --ignore-not-found=true --request-timeout=20s || true
          timeout 60 kubectl delete ns datadog --ignore-not-found=true --request-timeout=20s || true
          timeout 60 kubectl delete ns external-secrets --ignore-not-found=true --request-timeout=20s || true

          echo "=========================================="
          echo "Checking LoadBalancer services again..."
          echo "=========================================="
          timeout 30 kubectl get svc -A --request-timeout=20s | grep LoadBalancer || echo "No LoadBalancer services found ✅"

      - name: Cleanup AWS Load Balancers (Force Delete)
        if: ${{ github.event.inputs.action == 'destroy' }}
        run: |
          set +e

          echo "=========================================="
          echo "Searching for Classic ELBs..."
          echo "=========================================="

          ELBS=$(timeout 60 aws elb describe-load-balancers --region "$AWS_REGION" --query 'LoadBalancerDescriptions[].LoadBalancerName' --output text 2>/dev/null)

          if [ -z "$ELBS" ]; then
            echo "No Classic ELBs found ✅"
          else
            echo "Found Classic ELBs:"
            echo "$ELBS"
            for elb in $ELBS; do
              echo "Deleting Classic ELB: $elb"
              timeout 60 aws elb delete-load-balancer --load-balancer-name "$elb" --region "$AWS_REGION" || true
            done
          fi

          echo "=========================================="
          echo "Searching for ALB/NLB..."
          echo "=========================================="

          LBS=$(timeout 60 aws elbv2 describe-load-balancers --region "$AWS_REGION" --query 'LoadBalancers[].LoadBalancerArn' --output text 2>/dev/null)

          if [ -z "$LBS" ]; then
            echo "No ALB/NLB found ✅"
          else
            echo "Found ALB/NLB:"
            echo "$LBS"
            for lb in $LBS; do
              echo "Deleting LB ARN: $lb"
              timeout 60 aws elbv2 delete-load-balancer --load-balancer-arn "$lb" --region "$AWS_REGION" || true
            done
          fi

          echo "=========================================="
          echo "Waiting 60 seconds for TargetGroups cleanup..."
          echo "=========================================="
          sleep 60

          echo "=========================================="
          echo "Deleting leftover Target Groups..."
          echo "=========================================="

          TGS=$(timeout 60 aws elbv2 describe-target-groups --region "$AWS_REGION" --query 'TargetGroups[].TargetGroupArn' --output text 2>/dev/null)

          if [ -z "$TGS" ]; then
            echo "No Target Groups found ✅"
          else
            for tg in $TGS; do
              echo "Deleting TargetGroup: $tg"
              timeout 60 aws elbv2 delete-target-group --target-group-arn "$tg" --region "$AWS_REGION" || true
            done
          fi

          echo "=========================================="
          echo "AWS LoadBalancer cleanup finished ✅"
          echo "=========================================="

      - name: Terraform Destroy (Full)
        if: ${{ github.event.inputs.action == 'destroy' }}
        run: |
          set +e

          echo "=========================================="
          echo "Destroying EVERYTHING using terraform destroy..."
          echo "=========================================="

          terraform destroy -lock-timeout=15m -auto-approve -var-file=nonprod.tfvars

          echo "=========================================="
          echo "Destroy Completed ✅"
          echo "=========================================="

      - name: Force Remove Terraform Lock (if stuck)
        if: ${{ github.event.inputs.action == 'destroy' }}
        run: |
          set +e

          echo "=========================================="
          echo "Cleaning DynamoDB terraform locks (force)"
          echo "=========================================="

          aws dynamodb delete-item \
            --table-name terraform-locks \
            --key '{"LockID":{"S":"cloud-native-devops-nti-tf-state/infra/terraform.tfstate"}}' \
            --region "$AWS_REGION" || true

          aws dynamodb delete-item \
            --table-name terraform-locks \
            --key '{"LockID":{"S":"cloud-native-devops-nti-tf-state/infra/terraform.tfstate-md5"}}' \
            --region "$AWS_REGION" || true

          echo "Terraform lock cleanup done ✅"
